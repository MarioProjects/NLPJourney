{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Fine-tuning  - Maths OCR\n",
    "\n",
    "- https://docs.unsloth.ai/basics/vision-fine-tuning\n",
    "- https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel, is_bf16_supported # FastLanguageModel for LLMs\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TextStreamer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# To render latex in jupyter\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We support Llama 3.2 Vision 11B, 90B; Pixtral; Qwen2VL 2B, 7B, 72B; and any Llava variant like Llava NeXT!\n",
    "* We support 16bit LoRA via `load_in_4bit=False` or 4bit QLoRA. Both are accelerated and use much less memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2_Vl patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.168 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    # More models at https://huggingface.co/unsloth\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
    "\n",
    "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.visual` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We'll be using a sampled dataset of handwritten maths formulas. The goal is to convert these images into a computer readable form - ie in LaTeX form, so we can render it. This can be very useful for complex formulas.\n",
    "\n",
    "You can access the dataset [here](https://huggingface.co/datasets/unsloth/LaTeX_OCR). The full dataset is [here](https://huggingface.co/datasets/linxy/LaTeX_OCR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text'],\n",
       "    num_rows: 68686\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"unsloth/LaTeX_OCR\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an overview look at the dataset. We shall see what the 3rd image is, and what caption it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \\frac { N } { M } } \\in { \\bf Z } , { \\frac { M } { P } } \\in { \\bf Z } , { \\frac { P } { Q } } \\in { \\bf Z }\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle { \\frac { N } { M } } \\in { \\bf Z } , { \\frac { M } { P } } \\in { \\bf Z } , { \\frac { P } { Q } } \\in { \\bf Z }$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can render the LaTeX in the browser directly!\n",
    "latex = dataset[0][\"text\"]\n",
    "print(latex)\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the input for the model\n",
    "\n",
    "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\",  \"text\": Q},\n",
    "            {\"type\": \"image\", \"image\": image}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\",  \"text\": A}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        { \n",
    "            \"role\": \"user\",\n",
    "            \"content\" : [\n",
    "                {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "                {\"type\" : \"image\", \"image\" : sample[\"image\"]}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : [\n",
    "                {\"type\" : \"text\",  \"text\"  : sample[\"text\"]}\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dataset into the \"correct\" format for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write the LaTeX representation for this image.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "{ \\frac { N } { M } } \\in { \\bf Z } , { \\frac { M } { P } } \\in { \\bf Z } , { \\frac { P } { Q } } \\in { \\bf Z }<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.apply_chat_template(converted_dataset[0][\"messages\"], tokenize=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference before finetuning\n",
    "\n",
    "Let's first see before we do any finetuning what the model outputs for the first example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready for inference!\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "print(\"Model is ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[0][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write the LaTeX representation for this image.<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, temperature=0.1, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathbf { N } _ { M } \\in \\mathbf { Z } , \\; \\; \\; \\mathbf { M } _ { P } \\in \\mathbf { Z } , \\; \\; \\; \\mathbf { P } _ { Q } \\in \\mathbf { Z }$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decode the output, from ids to text\n",
    "preds = tokenizer.batch_decode(outputs)\n",
    "pred = preds[0]  # We sent just one example, get the first one\n",
    "# from the generated text, cut the prompt\n",
    "prompt_ids = inputs[\"input_ids\"][0]  # We sent just one example, get the first one\n",
    "prompt_len = len(prompt_ids)\n",
    "pred = pred[prompt_len:]\n",
    "# get all between \"$$\"\" and \"$$\"\n",
    "latex = pred.split(\"$$\")[1]\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 30 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
    "\n",
    "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "print(\"Model is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = converted_dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 68,686 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 50,855,936/5,063,203,328 (1.00% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 01:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.323700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.514500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"handtolatext-lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"handtolatext-lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference after finetuning\n",
    "\n",
    "Let's see after finetuning what the model outputs for the first example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready for inference!\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "print(\"Model is ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[0][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, temperature=0.1, min_p=0.1)\n",
    "# Decode the output, from ids to text\n",
    "preds = tokenizer.batch_decode(outputs)\n",
    "pred = preds[0]  # We sent just one example, get the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write the LaTeX representation for this image.<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\\frac { N } { M } \\in \\mathbf { Z } , \\frac { M } { P } \\in \\mathbf { Z } , \\frac { P } { Q } \\in \\mathbf { Q }<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac { N } { M } \\in \\mathbf { Z } , \\frac { M } { P } \\in \\mathbf { Z } , \\frac { P } { Q } \\in \\mathbf { Q }$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The output follows the training format\n",
    "# We need to cut from last \"<|im_start|>assistant\\n\" to the end\n",
    "pred = pred.split(\"<|im_start|>assistant\\n\")[1]\n",
    "# And remove the last \"<|im_end|>\"\n",
    "# The training dataset don't use the $$ delimiters\n",
    "latex = pred.split(\"<|im_end|>\")[0]\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

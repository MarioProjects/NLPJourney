{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Supervised Fine-Tuning\n",
    "\n",
    "Supervised Fine-Tuning (SFT) is a process primarily used to adapt pre-trained language models to follow instructions, engage in dialogue, and use specific output formats. This is typically done by training on datasets of human-written conversations and instructions.\n",
    "\n",
    "\n",
    "## When to Use SFT\n",
    "\n",
    "As a first step, you should consider whether using an existing instruction-tuned model with well-crafted prompts would suffice for your use case. SFT involves significant computational resources and engineering effort, so it should only be pursued when prompting existing models proves insufficient.\n",
    "\n",
    "> Consider SFT only if you: - Need additional performance beyond what prompting can achieve - Have a specific use case where the cost of using a large general-purpose model outweighs the cost of fine-tuning a smaller model - Require specialized output formats or domain-specific knowledge that existing models struggle with\n",
    "\n",
    "### Template Control\n",
    "\n",
    "SFT allows precise control over the model’s output structure. This is particularly valuable when you need the model to:\n",
    "\n",
    "- Generate responses in a specific chat template format\n",
    "- Follow strict output schemas\n",
    "- Maintain consistent styling across responses\n",
    "\n",
    "### Domain Adaptation\n",
    "\n",
    "When working in specialized domains, SFT helps align the model with domain-specific requirements by:\n",
    "\n",
    "- Teaching domain terminology and concepts\n",
    "- Enforcing professional standards\n",
    "- Handling technical queries appropriately\n",
    "- Following industry-specific guidelines\n",
    "\n",
    "> Before starting SFT, evaluate whether your use case requires: - Precise output formatting - Domain-specific knowledge - Consistent response patterns - Adherence to specific guidelines. This evaluation will help determine if SFT is the right approach for your needs.\n",
    "\n",
    "\n",
    "## Dataset Preparation\n",
    "\n",
    "The supervised fine-tuning process **requires a task-specific dataset** structured with input-output pairs. Each pair should consist of:\n",
    "\n",
    "- An input prompt\n",
    "- The expected model response\n",
    "- Any additional context or metadata\n",
    "\n",
    "The **quality of your training data is crucial** for successful fine-tuning. Let’s look at how to prepare and validate your dataset:\n",
    "\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "The success of your fine-tuning depends heavily on choosing the right training parameters. Let’s explore each important parameter and how to configure them effectively:\n",
    "\n",
    "The SFTTrainer configuration requires consideration of several parameters that control the training process. Let’s explore each parameter and their purpose:\n",
    "\n",
    "1. **Training Duration Parameters**:\n",
    "  - `num_train_epochs`: Controls total training duration\n",
    "  - `max_steps`: Alternative to epochs, sets maximum number of training steps\n",
    "  - More epochs allow better learning but risk overfitting\n",
    "\n",
    "2. **Batch Size Parameters**:\n",
    "  - `per_device_train_batch_size`: Determines memory usage and training stability\n",
    "  - `gradient_accumulation_steps`: Enables larger effective batch sizes\n",
    "  - Larger batches provide more stable gradients but require more memory\n",
    "\n",
    "\n",
    "3. **Learning Rate Parameters**:\n",
    "  - `learning_rate`: Controls size of weight updates\n",
    "  - `warmup_ratio`: Portion of training used for learning rate warmup\n",
    "  - Too high can cause instability, too low results in slow learning\n",
    "\n",
    "4. **Monitoring Parameters**:\n",
    "  - `logging_steps`: Frequency of metric logging\n",
    "  - `eval_steps`: How often to evaluate on validation data\n",
    "  - `save_steps`: Frequency of model checkpoint saves\n",
    "\n",
    "> Start with conservative values and adjust based on monitoring: - Begin with 1-3 epochs - Use smaller batch sizes initially - Monitor validation metrics closely - Adjust learning rate if training is unstable\n",
    "\n",
    "\n",
    "## Implementation with TRL\n",
    "\n",
    "Now that we understand the key components, let’s implement the training with proper validation and monitoring. We will use the `SFTTrainer` class from the Transformers Reinforcement Learning (TRL) library, which is built on top of the `transformers` library. Here’s a complete example using the TRL library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Set up the chat format](https://huggingface.co/docs/trl/sft_trainer#add-special-tokens-for-chat-format). The `setup_chat_format()` function in trl easily sets up a model and tokenizer for conversational AI tasks. This function:\n",
    "\n",
    "- Adds special tokens to the tokenizer, e.g. `<|im_start|>` and `<|im_end|>`, to indicate the start and end of a conversation.\n",
    "- Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "- Sets the `chat_template` of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\n",
    "    path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hello! How can I help you today?<|im_end|>\n",
      "<|im_start|>user\n",
      "I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.<|im_end|>\n",
      "<|im_start|>user\n",
      "That sounds great. Are there any resorts in the Caribbean that are good for families?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.<|im_end|>\n",
      "<|im_start|>user\n",
      "Okay, I'll look into those. Thanks for the recommendations!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "You're welcome. I hope you find the perfect resort for your vacation.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(dataset[\"train\"][0][\"messages\"], tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "<|im_start|>user\n",
      "Write a haiku about programming<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "Write a haiku about programming\n",
      "assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Daset format support](https://huggingface.co/docs/trl/sft_trainer#dataset-format-support). The `SFTTrainer` supports popular dataset formats. This allows you to pass the dataset to the trainer without any pre-processing directly. The following formats are supported:\n",
    "\n",
    "- conversational format\n",
    "```python\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "- instruction format\n",
    "```python\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "If your dataset uses one of the above formats, you can **directly pass it to the trainer without pre-processing**. The SFTTrainer will then format the dataset for you using the defined format from the model’s tokenizer with the apply_chat_template method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 04:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>1.158982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.111600</td>\n",
       "      <td>1.124065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.062400</td>\n",
       "      <td>1.095485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.048200</td>\n",
       "      <td>1.079698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>1.070457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.029200</td>\n",
       "      <td>1.061472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.003400</td>\n",
       "      <td>1.054751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.006500</td>\n",
       "      <td>1.050794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.021100</td>\n",
       "      <td>1.042638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.076200</td>\n",
       "      <td>1.033725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.990700</td>\n",
       "      <td>1.028269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>1.033091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>1.029898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.753800</td>\n",
       "      <td>1.031148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>1.028455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>1.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.776800</td>\n",
       "      <td>1.025296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>1.022838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>1.021673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>1.021487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.9605710778236389, metrics={'train_runtime': 244.7225, 'train_samples_per_second': 16.345, 'train_steps_per_second': 4.086, 'total_flos': 587568496250880.0, 'train_loss': 0.9605710778236389})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(f\"checkpoints/{finetune_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "<|im_start|>user\n",
      "Which is the capital of paris?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is the capital of paris?<|im_start|>\n",
      "<|im_start|>assistant\n",
      "Which is\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on the same prompt\n",
    "\n",
    "# Let's test the base model before training\n",
    "prompt = \"Which is the capital of paris?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Training Progress\n",
    "\n",
    "Effective monitoring is crucial for successful fine-tuning. Let’s explore what to watch for during training.\n",
    "\n",
    "### Understanding Loss Patterns\n",
    "\n",
    "Training loss typically follows three distinct phases:\n",
    "\n",
    "1. **Initial Sharp Drop**: Rapid adaptation to new data distribution\n",
    "2. **Gradual Stabilization**: Learning rate slows as model fine-tunes\n",
    "3. **Convergence**: Loss values stabilize, indicating training completion\n",
    "\n",
    "\n",
    "### Metrics to Monitor\n",
    "\n",
    "Effective monitoring involves tracking quantitative metrics, and evaluating qualitative metrics. Available metrics are:\n",
    "\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Learning rate progression\n",
    "- Gradient norms\n",
    "\n",
    "> Watch for these **warning signs during** training: 1. Validation loss increasing while training loss decreases (overfitting) 2. No significant improvement in loss values (underfitting) 3. Extremely low loss values (potential memorization) 4. Inconsistent output formatting (template learning issues).\n",
    "\n",
    "\n",
    "### The Path to Convergence\n",
    "\n",
    "As training progresses, the loss curve should gradually stabilize. The key indicator of healthy training is a **small gap between training and validation loss**, suggesting the model is **learning generalizable patterns** rather than memorizing specific examples. The absolute loss values will vary depending on your task and dataset.\n",
    "\n",
    "![Perfect Convergence](misc/sft_perfect_convergence.png \"Perfect Convergence\")\n",
    "\n",
    "\n",
    "### Warning Signs to Watch For\n",
    "\n",
    "Several patterns in the loss curves can indicate potential issues. Below we illustrate common warning signs and solutions that we can consider.\n",
    "\n",
    "![Bad Validation Curve](misc/sft_bad_validation.png \"Bad Validation Curve\")\n",
    "\n",
    "If the validation loss decreases at a significantly slower rate than training loss, your model is **likely overfitting** to the training data. Consider:\n",
    "\n",
    "- Reducing the training steps\n",
    "- Increasing the dataset size\n",
    "- Validating dataset quality and diversity\n",
    "\n",
    "![No Learning](misc/sft_no_learning.png \"No Learning\")\n",
    "\n",
    "If the loss doesn’t show significant improvement, the model might be:\n",
    "\n",
    "- Learning too slowly (try increasing the learning rate)\n",
    "- Struggling with the task (check data quality and task complexity)\n",
    "- Hitting architecture limitations (consider a different model)\n",
    "\n",
    "![Memorization](misc/sft_memorization.png \"Memorization\")\n",
    "\n",
    "Extremely low loss values could **suggest memorization** rather than learning. This is particularly concerning if:\n",
    "\n",
    "- The model performs poorly on new, similar examples\n",
    "- The outputs lack diversity\n",
    "- The responses are too similar to training examples\n",
    "\n",
    "> Regular qualitative evaluation of the model's responses helps catch issues that metrics alone might miss.\n",
    "\n",
    "We should note that the interpretation of the loss values we outline here is aimed on the most common case, and in fact, loss values can behave on various ways depending on the model, the dataset, the training parameters, etc. If you interested in exploring more about outlined patterns, you should check out this blog post by the people at [Fast AI](https://www.fast.ai/posts/2023-09-04-learning-jumps/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
